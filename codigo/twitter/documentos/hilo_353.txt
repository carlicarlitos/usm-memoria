So, there's a thought experiment of sorts called "Roko's Basilisk" and if you are deeply upset by losing the game or being made aware of your tongue, you might want to mute me for an hour. 
 The short version of the Basilisk is that if you believe, as some people do, that it's inevitable our technological progression will give rise to an all-powerful artificially intelligent entity, then you should do everything you can to make sure that happens, for your safety. 
 Because the entity, being all-powerful, will be able to punish anyone who stood in the way of its existence. 
 You can read more about it in detail here. https://t.co/w56CxKOtoq 
 There are a lot of wrinkles and permutations to it. For instance, it assumes the "Basilisk" AI is *friendly* to human existence, or else it would have no reason to punish specific humans; it would ignore us and likely wipe us out obliquely as it pursued its own purposes. 
 It also assumes that a friendly God-AI, seeking the most good for the most people, would want to have ensured its existence happens at the earliest point possible, to do the most good. 
 And since there is no demonstrated ability to act in the past in a sci-fi time travel scenario and there is likely never to be such a capability even for an "all-powerful AI", the threat of future action would be its mechanism for influencing us now. 
 I should say I don't find any of this compelling. It's a humanist/transhumanist's version of Pascal's Wager, and it contains the same fundamental error as Pascal did: it's not an either/or choice, you're betting on a specific number on a very large roulette wheel. 
 The idea that a machine could be so far advanced beyond us as to be like unto a literal god but that we could fathom its motives and meanings enough to predict its future punishment of us is farcical. 
 You might also be thinking, "Pascal's wager assumes we're all going to face judgment after we're dead. Without an afterlife, dead is dead. Most of us will be beyond the power of the AI to punish us." 
 Well, here's where the idea gets really incredible. And by that I mean, "not credible". Because the all-powerful AI will be able to simulate you in perfect detail, and torture that simulation. Which is the same as torturing you. https://t.co/o0dpze8WVg 
 Now, the titular "Basilisk" is not the AI itself. It's the idea, the meme, that can take root in your software, that you must devote yourself to bringing about This Very Specific Technological Singularity, or you--some version of you--will be tortured forever. 
 I've observed on here before that one of the fundamental flaws of human psychology is we have a hard time actually seeing literal future versions of ourselves as ourselves. 
 When you put off doing the dishes until tomorrow, by and large you aren't actually thinking of the reality of doing them tomorrow. You're feeling that this means you don't have to do the dishes. 
 "A computer will simulate you down to every subatomic particle, and this simulation will have thought everything you've thought and react to every situation the same way you'd react and it will be indistinguishable from you in every way. Care very much about what happens to it." 
 I can't bring myself to do that. I don't know. Maybe I'm the weird one and there's something broken in me. I also don't care if I just lost the game and no one can make me acutely aware of my tongue. 
 Even allowing that the simulated me is alive and sapient and human (which I can fully grant), it's got nothing to do with me in particular. A computer I can't fathom decided to torture it for reasons that will never make sense to me, after I'm dead. I have no power to stop it. 
 Oh, but they are assuming the AI is friendly. The community that dreamed this up imagines creating an AI that will save the world. https://t.co/FUlTvUMBB7 
 Intrinsic in the Basilisk is the notion that creating a friendly AI is *a moral imperative* because it will head off human suffering and extinction, including other, less friendly AIs. 
 Their definition of "friendly" is obviously specialized and sharply utilitarian. 
 And since the God-AI is so good and necessary, doing anything to slow down its creation is evil and harmful. And that includes doing less than you can to bring it about. (Like donating less money to an institute dedicated to creating it than you possibly could.) 
 The AI has no "rational" reason to punish people who didn't learn of and believe in the Basilisk notion, so it wouldn't But by that logic, wouldn't it also punish people who didn't spread the Basilisk every chance they got? 
 I'm just fascinated by the way these rationalists/materialists have so completely recapitulated the religion that most of them grew up in or around. 
 They've imagined an all-powerful, all-knowing, and loving God, who will lovingly punish everybody who doesn't serve it. You can placate it by contributing labor and money to an institution founded in the God's name. 
 And as @arthur_affect notes in this subthread, some permutations of it even have Calvinism-style predestination. We're all either already righteous and saved or not and condemned. https://t.co/eALtPM4Oh4 
 @arthur_affect I think it was meant to critique some of the flaws in the assumptions it relies on, yes, but like the similarly-purposed Schrodinger's Cat, it went on to be used as a way of teaching that which it critiqued. https://t.co/y1dCjkQycF 
 I think people who don't believe the premises are flawed would probably argue that its creator's intentions don't matter, if the logic is sound (as they believe it to be.) 
 Compare also the transhumanists trying to "live long enough to live forever" (part of the same crowd who promulgate the Basilisk) with the rapturists who earnestly pray to be the last generation, to enter into glorious immortal existence without dying. https://t.co/oyAu37pyaa 
 Anyway. Please don't lose any sleep worrying about the Basilisk. There are legitimate reasons to be wary of AI. Most of them have to do with the way we offload power and responsibility to algorithms and then act like this has neither victims nor perpetrators. 
 And I'm going to end the thread there because I told people to mute me for an hour, then didn't bother keeping track of time. :P